# Related & Previous Works 

## Screen Parsing and GUI Grounding

The idea of using object detection to parse GUI screenshots predates VLM-based agents. OmniParser [9] (Microsoft, 2024) is the most direct predecessor to SCALP's perception pipeline. It combines a YOLOv8 model fine-tuned on 67k screenshots for interactive region detection, an OCR module for text extraction, and a fine-tuned BLIP-v2 (later Florence-2) model for icon captioning. OmniParser improved GPT-4V's grounding accuracy on the ScreenSpot benchmark from 16.2% to 73.0% [9], demonstrating that dedicated parsing modules dramatically improve VLM agent performance. However, OmniParser is purely a perception module and does not provide any agent loop, action execution, state tracking, and managing downstream context to the LLM. It outputs all detected elements (bounding boxes, descriptions, OCR text) as a monolithic dump, with no filtering or token budgeting. On complex screens with many interactive elements, this can consume a substantial portion of the LLM's context window with low-signal information. Furthermore, OmniParser still requires a VLM downstream, where it passes both an annotated screenshot image and structured text to GPT-4V, and the vision model is not eliminated from the pipeline but merely augmented.

OmniParser V2 [9] (February 2025) reduced latency by 60% (0.6s/frame on A100) and introduced OmniTool, a dockerized action execution layer. Even with OmniTool, the system lacks intelligent context management and the underlying model remains a perception-to-VLM relay with no optimization for what the LLM actually needs at each decision step.

Daneshvar and Wang [10] evaluated four YOLO architectures (YOLOv5, v6, v7, v8) on the VINS mobile GUI dataset, finding that all models exceeded 93% mAP@0.5. Their work validated YOLO as a competitive detector for GUI elements but was limited to mobile interfaces and did not integrate the detector into an agent system. SeeClick [6] introduced the ScreenSpot benchmark and demonstrated that GUI grounding pre-training on a 9.6B-parameter VLM outperformed the 18B CogAgent, but it couples perception and reasoning in a single model. There is no separation between "seeing" the screen and "deciding" what to do, making it impossible to optimize either component independently. UI-JEPA [21] applied self-supervised joint embedding prediction to learn abstract UI representations from unlabeled video, achieving a 50.5x computational cost reduction over large VLMs for intent prediction. While UI-JEPA validates that lightweight, domain-specific models can outperform general VLMs on UI understanding, it addresses only intent classification and does not extend to action execution or multi-step agent loops.

## Compositional Agent Architectures

Agent S2 [11] represents the state-of-the-art in compositional agent design. Its Mixture-of-Grounding (MoG) approach delegates visual grounding to UI-TARS-72B, text grounding to Tesseract OCR, and structural grounding to a programmatic interface, while a Claude-3.7-Sonnet manager handles high-level planning. This compositional design achieved 27.0% on the OSWorld 15-step benchmark. However, Agent S2 remains expensive, requiring a 72b-parameter vision model for grounding plus a frontier LLM for reasoning. It operates exclusively through GUI interactions (click, type, scroll) with no programmatic action fallback, and its most frequent failure mode is planning errors caused by noisy subtask information [11]. Critically, it provides no human feedback channel beyond the initial task specification.

ActionEngine [22] (Georgia Tech / Microsoft, 2025) takes a different approach: a crawling agent builds a state-machine graph of GUI transitions offline, and an execution agent synthesizes complete Python programs for task completion, achieving 95% success with a single LLM call on Reddit tasks from WebArena, an 11.8x cost reduction over reactive agents. This validates that reducing per-step LLM calls through programmatic execution dramatically improves both cost and reliability. However, ActionEngine requires an expensive offline crawling phase and is limited to web applications.

## Context Engineering and Agent Harness Design

A growing body of work identifies context engineering, the design of what information reaches the LLM and how, as the primary bottleneck in agent performance, distinct from and often more important than model capability. Empirical studies on the APEX-Agents benchmark show that frontier models achieve only 24% accuracy on professional agent tasks despite exceeding 90% on traditional benchmarks [23], with failures attributed to orchestration problems rather than reasoning gaps. Five essential harness components have been identified: context management, tool selection, error recovery, state management, and external memory [23].

Research on context management strategies reveals that simple observation masking, omitting all but the most recent observations, can match or outperform LLM-based summarization while being over 50% cheaper, because observation tokens dominate agentic traces and chain-of-thought reasoning is preserved even when intermediate observations are dropped [24]. Separately, reducing the number of exposed tools has been shown to improve both success rate and token efficiency: Vercel demonstrated that collapsing 15 specialized tools into 2 general-purpose tools (bash + file access) improved task success from 80% to 100% while reducing tokens by 37% [25].

These findings directly inform SCALP's harness design. Existing systems like OmniParser and OmniTool push all perceptual output to the LLM indiscriminately: every detected element, every OCR result, every icon description is included regardless of relevance to the current task step. SCALP's MCP server architecture inverts this pattern through **pull-based context delivery**: the LLM calls `capture_and_detect()` only when it needs fresh screen state, `get_brain_signal()` only at decision points, and `get_action_history()` only when reviewing past actions. Each tool returns a compact structured JSON payload, a list of `{type, label, position, confidence}` objects, rather than a verbose natural-language description or annotated image. In a constrained application with 5â€“10 UI element classes, a single screen parse produces on the order of tens of tokens, compared to the thousands consumed by a full screenshot in a VLM pipeline or an exhaustive DOM dump.

SCALP further optimizes context through a **CLI-first execution hierarchy**. When a task can be accomplished programmatically, such as launching an application, executing a shell command, interacting with a web page via Playwright, the agent bypasses the YOLO + OCR perception pipeline entirely and uses `run_command()` or `browser_action()` directly. GUI perception is reserved for surfaces that lack programmatic interfaces and for post-action verification via `verify_screen()`. This fallback hierarchy, enforced through tool descriptions in the MCP schema, reduces the number of perception-heavy LLM calls per task. The effect is analogous to ActionEngine's programmatic execution [22], but achieved without an offline crawling phase and generalized beyond web applications.

## MCP Server as Agent Infrastructure

The Model Context Protocol [20], introduced by Anthropic in November 2024 and adopted by OpenAI in March 2025, standardizes how LLMs integrate with external tools and data sources. To date, MCP has been used primarily for data retrieval and developer tooling (GitHub, Slack, database connectors), with limited application to computer use agents. OmniMCP [26] (OpenAdaptAI) is the closest existing system: it wraps OmniParser in an MCP interface and adds action execution. However, OmniMCP bundles perception, planning, and execution into a monolithic pipeline rather than exposing them as composable, independently callable tools the LLM cannot selectively query screen state without triggering the full parsing chain.

SCALP's MCP server exposes fine-grained, independently callable tools: `capture_and_detect()` for perception, `get_brain_signal()` for brain feedback, `click()` / `type_text()` for GUI actions, `run_command()` for programmatic actions, `verify_screen()` for post-action confirmation, and `get_action_history()` for state review. This modular design means the LLM agent decides what to perceive and when, rather than receiving a predetermined bundle of information. It also means the system is model-agnostic: any MCP-compatible client can operate the agent without modification.

The MCP request-response boundary does introduce a constraint for the EEG integration: brain signals are a continuous stream, but MCP tools are invoked on demand. SCALP resolves this through a standard sensor-system pattern [20]: the EEG hardware feeds a ring buffer inside the server process, and the `get_brain_signal()` tool samples the latest N-second window, runs the encoder, and returns a discrete label. The continuous stream lives inside the server; the protocol boundary remains request-response. This is the same pattern used in audio processing, environmental monitoring, and IoT sensor APIs.